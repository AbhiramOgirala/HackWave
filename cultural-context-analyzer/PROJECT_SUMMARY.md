# Cultural Context Analyzer - Project Summary

## Overview

The **Cultural Context Analyzer** is a full-stack web application that helps students and educators understand cultural and historical references in literature and texts. It uses Google's Gemini AI combined with NLP-powered entity detection to provide comprehensive, interactive cultural analysis.

## Problem Statement

Literature and historical texts often contain cultural references that students miss. This tool bridges that gap by:
- Identifying cultural origins
- Explaining cross-cultural connections
- Providing modern analogies with contemporary references (Gen Z/Millennial focused)
- Interactive timelines and geographic mapping
- Automatic entity detection with Wikipedia-powered explanations
- Visual learning through color-coded highlights

## Solution Architecture

### Technology Stack

**Backend:**
- **FastAPI**: High-performance Python web framework
- **Supabase (PostgreSQL)**: Cloud database for data persistence
- **Google Gemini API**: AI-powered cultural analysis (gemini-2.5-flash model)
- **spaCy NLP**: Named Entity Recognition for cultural entities
- **Wikipedia API**: Verified entity enrichment
- **Python 3.9+**: Core programming language

**Frontend:**
- **React 18**: Modern UI library
- **Vite**: Fast build tool and dev server
- **TailwindCSS**: Utility-first CSS framework
- **Lucide React**: Beautiful icon library
- **Axios**: HTTP client for API calls
- **ReactMarkdown**: Markdown rendering support

**Database:**
- **Supabase (PostgreSQL)**: Cloud-hosted database with JSONB support for enhanced features

## Key Features

### Core Analysis Features

1. **Cultural Origin Analysis**: Identifies the primary culture, time period, and geographical context
2. **Cross-Cultural Connections**: Shows how the concept relates to or influenced other cultures
3. **Modern Analogies**: Contemporary parallels with Gen Z/Millennial references (social media, gaming, tech)
4. **Multi-Language Support**: 12+ languages (English, Hindi, Spanish, French, German, Chinese, Japanese, Arabic, Bengali, Tamil, Telugu, Marathi)

### Enhanced Interactive Features (v2.0)

5. **üìÖ Interactive Historical Timelines**
   - Chronological event display with year, title, description, significance
   - Expandable/collapsible interface
   - Conditional generation (only for historical content)
   - Focus on cultural periods, NOT author biography

6. **üó∫Ô∏è Geographic Mapping**
   - GPS coordinates for cultural locations
   - Google Maps integration with direct links
   - Historical vs. modern location names
   - Cultural significance explanations
   - Conditional generation (only when places are relevant)

7. **üìñ Key Concept Explainers**
   - Pop-out modal dialogs for cultural terms
   - Definition, context, and modern parallels
   - Click-to-expand interface
   - Only generated for complex cultural/technical terms

8. **üîó External Resources**
   - Curated educational links (Khan Academy, National Geographic, etc.)
   - Interactive timelines and maps
   - Educational videos and further reading
   - **Only real, verified URLs** included

### NLP-Powered Entity Detection (v3.0)

9. **üéØ Automatic Cultural Entity Detection**
   - spaCy NER detects PERSON, ORG, GPE, EVENT, WORK_OF_ART, FAC, NORP, LANGUAGE
   - Wikipedia API enrichment with verified summaries
   - Interactive text highlights with tooltips
   - 9 cultural significance categories (mythological, historical, literary, philosophical, religious, artistic, geographical, biographical, general)
   - Color-coded highlighting by significance
   - Entity caching for performance (30-day TTL)

10. **Analysis History & Persistence**
    - Supabase cloud storage for all analyses
    - Quick access to previous analyses
    - Delete functionality
    - JSONB columns for enhanced data

11. **Modern UI/UX**
    - Responsive design (mobile-friendly)
    - Real-time loading states
    - Beautiful gradient designs
    - Interactive highlights and tooltips
    - Smooth animations
    - Entity legends and summaries

## Project Structure

```
cultural-context-analyzer/
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ main.py                    # FastAPI app & routes (with NLP endpoints)
‚îÇ   ‚îú‚îÄ‚îÄ database.py                # Supabase client & entity caching
‚îÇ   ‚îú‚îÄ‚îÄ gemini_service.py          # Gemini API integration
‚îÇ   ‚îú‚îÄ‚îÄ nlp_service.py             # spaCy entity extraction & enrichment
‚îÇ   ‚îú‚îÄ‚îÄ wikipedia_service.py       # Wikipedia/Wikidata API integration
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îÇ   ‚îú‚îÄ‚îÄ entity_cache_migration.sql # Entity cache table schema
‚îÇ   ‚îú‚îÄ‚îÄ .env.example              # Environment template
‚îÇ   ‚îî‚îÄ‚îÄ venv/                     # Virtual environment (created)
‚îÇ
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.jsx               # Main React component (enhanced features)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.jsx              # React entry point
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.css             # TailwindCSS styles
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ components/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ EntityHighlight.jsx  # Entity highlighting components
‚îÇ   ‚îú‚îÄ‚îÄ index.html                # HTML template
‚îÇ   ‚îú‚îÄ‚îÄ package.json              # Node dependencies
‚îÇ   ‚îú‚îÄ‚îÄ vite.config.js            # Vite configuration
‚îÇ   ‚îú‚îÄ‚îÄ tailwind.config.js        # TailwindCSS config
‚îÇ   ‚îú‚îÄ‚îÄ postcss.config.js         # PostCSS config
‚îÇ   ‚îú‚îÄ‚îÄ .env.example              # Environment template
‚îÇ   ‚îî‚îÄ‚îÄ node_modules/             # Dependencies (created)
‚îÇ
‚îú‚îÄ‚îÄ README.md                      # Main documentation
‚îú‚îÄ‚îÄ SETUP_GUIDE.md                # Detailed setup instructions
‚îú‚îÄ‚îÄ QUICKSTART.md                 # Quick start guide
‚îú‚îÄ‚îÄ SUPABASE_SETUP.md             # Supabase setup guide
‚îú‚îÄ‚îÄ ENHANCED_FEATURES.md          # v2.0 enhanced features documentation
‚îú‚îÄ‚îÄ NLP_ENRICHMENT_GUIDE.md       # NLP module documentation
‚îú‚îÄ‚îÄ NLP_QUICKSTART.md             # NLP quick start
‚îú‚îÄ‚îÄ PROJECT_SUMMARY.md            # This file
‚îú‚îÄ‚îÄ supabase_setup.sql            # Main database schema
‚îú‚îÄ‚îÄ remove_visualization_migration.sql  # Schema updates
‚îú‚îÄ‚îÄ setup.ps1                     # Automated setup script
‚îú‚îÄ‚îÄ setup-nlp.ps1                 # NLP-specific setup script
‚îî‚îÄ‚îÄ .gitignore                    # Git ignore rules
```

## API Endpoints

### Core Analysis

#### POST /api/analyze
Analyzes text for cultural context **with automatic NLP entity detection**

**Request:**
```json
{
  "text": "The Ramayana is an ancient Indian epic...",
  "language": "en"
}
```

**Response:**
```json
{
  "id": 1,
  "input_text": "The Ramayana is an ancient Indian epic...",
  "language": "en",
  "cultural_origin": "Detailed cultural origin analysis...",
  "cross_cultural_connections": "Cross-cultural connections...",
  "modern_analogy": "Modern analogy with Gen Z references...",
  "image_url": null,
  "timeline_events": [
    {
      "year": "1500-1200 BCE",
      "title": "Composition Period",
      "description": "...",
      "significance": "..."
    }
  ],
  "geographic_locations": [
    {
      "name": "Ayodhya",
      "coordinates": {"lat": 26.7922, "lng": 82.1998},
      "significance": "...",
      "modern_name": "Ayodhya, India"
    }
  ],
  "key_concepts": [
    {
      "term": "Dharma",
      "definition": "...",
      "context": "...",
      "modern_parallel": "..."
    }
  ],
  "external_resources": {
    "timeline_links": ["..."],
    "map_links": ["..."],
    "further_reading": ["..."]
  },
  "detected_entities": [
    {
      "text": "Ramayana",
      "type": "WORK_OF_ART",
      "start": 4,
      "end": 12,
      "summary": "Ancient Indian epic...",
      "url": "https://en.wikipedia.org/wiki/Ramayana",
      "cultural_significance": "mythological",
      "source": "Wikipedia"
    }
  ],
  "created_at": "2024-01-01T00:00:00"
}
```

### History Management

#### GET /api/history
Returns list of previous analyses (paginated, up to 20 per request)

#### GET /api/analysis/{id}
Retrieves specific analysis by ID

#### DELETE /api/analysis/{id}
Deletes specific analysis

### Statistics

#### GET /api/stats
Returns statistics about analyses (total count, language distribution)

### NLP Entity Endpoints (NEW)

#### POST /api/entities/extract
Extract and enrich cultural entities from text **on-demand** (without full Gemini analysis)

**Request:**
```json
{
  "text": "Achilles fought in the Trojan War"
}
```

**Response:**
```json
{
  "entities": [
    {
      "text": "Achilles",
      "type": "PERSON",
      "start": 0,
      "end": 8,
      "summary": "Greek mythological hero...",
      "cultural_significance": "mythological"
    }
  ],
  "total_detected": 2,
  "enriched_count": 2
}
```

#### GET /api/entities/highlights
Get entity highlights optimized for frontend display (query parameter: `text`)

**Response:**
```json
{
  "highlights": [
    {
      "start": 0,
      "end": 8,
      "text": "Achilles",
      "type": "PERSON",
      "tooltip": {
        "title": "Achilles",
        "summary": "...",
        "significance": "mythological",
        "url": "https://...",
        "source": "Wikipedia"
      }
    }
  ],
  "count": 1
}
```

## Database Schema

### Table: analyses

| Column | Type | Description |
|--------|------|-------------|
| id | INTEGER | Primary key (auto-increment) |
| input_text | TEXT | Original text input |
| language | VARCHAR(10) | Language code (e.g., 'en', 'hi') |
| cultural_origin | TEXT | Cultural origin analysis |
| cross_cultural_connections | TEXT | Cross-cultural connections |
| modern_analogy | TEXT | Modern analogy explanation |
| image_url | TEXT | Image prompt (visualization feature removed) |
| **timeline_events** | **JSONB** | **Array of historical events (v2.0)** |
| **geographic_locations** | **JSONB** | **Array of locations with coordinates (v2.0)** |
| **key_concepts** | **JSONB** | **Array of cultural concepts (v2.0)** |
| **external_resources** | **JSONB** | **Object with resource links (v2.0)** |
| **detected_entities** | **JSONB** | **Array of NLP-detected entities (v3.0)** |
| created_at | TIMESTAMP | Creation timestamp |

### Table: entity_cache (NEW in v3.0)

| Column | Type | Description |
|--------|------|-------------|
| id | BIGSERIAL | Primary key |
| entity_name | TEXT | Entity name (e.g., "Odysseus") |
| entity_type | TEXT | NER type (PERSON, ORG, GPE, etc.) |
| summary | TEXT | Wikipedia summary |
| url | TEXT | Wikipedia URL |
| categories | JSONB | Cultural categories |
| cultural_significance | TEXT | Classification (mythological, historical, etc.) |
| wikidata | JSONB | Wikidata metadata |
| source | TEXT | "Wikipedia" or "Wikidata" |
| created_at | TIMESTAMP | Cache timestamp |
| **UNIQUE(entity_name, entity_type)** | | **Prevents duplicates** |

**Indexes:**
- GIN index on `detected_entities` in analyses table
- Indexes on `entity_name`, `entity_type`, `created_at` in entity_cache
- Index on `created_at DESC` for analyses history

**Cultural Significance Categories:**
1. mythological - Mythology, folklore, legends
2. historical - Ancient/classical/medieval periods
3. literary - Books, poetry, novels
4. philosophical - Philosophy, thinkers
5. religious - Religious figures, texts
6. artistic - Art, music, paintings
7. geographical - Places, locations
8. biographical - People, organizations
9. general - Uncategorized

## Setup & Installation

### Quick Setup (Supabase Cloud Database)

1. **Create Supabase Project**
   - Sign up at [supabase.com](https://supabase.com)
   - Create new project
   - Copy credentials from Dashboard

2. **Run Database Setup**
   - Open Supabase SQL Editor
   - Run `supabase_setup.sql` for main schema
   - Run `backend/entity_cache_migration.sql` for NLP features

3. **Configure Backend** - Create `backend/.env`
   ```env
   GEMINI_API_KEY=your_gemini_api_key
   SUPABASE_URL=https://your-project.supabase.co
   SUPABASE_ANON_KEY=your_anon_key
   ```

4. **Install Backend Dependencies**
   ```powershell
   cd backend
   python -m venv venv
   .\venv\Scripts\Activate.ps1
   pip install -r requirements.txt
   
   # Install spaCy model for NLP
   python -m spacy download en_core_web_sm
   ```

5. **Configure Frontend** - Create `frontend/.env`
   ```env
   VITE_API_URL=http://localhost:8000
   ```

6. **Install Frontend Dependencies**
   ```powershell
   cd frontend
   npm install
   ```

### Automated Setup Scripts

**Full setup:**
```powershell
.\setup.ps1
```

**NLP-only setup:**
```powershell
.\setup-nlp.ps1
```

### Running the Application

**Terminal 1 - Backend:**
```powershell
cd backend
.\venv\Scripts\Activate.ps1
python main.py
```
Access at: http://localhost:8000

**Terminal 2 - Frontend:**
```powershell
cd frontend
npm run dev
```
Access at: http://localhost:5173

## Usage Examples

### Example 1: Indian Epic (All Features)
**Input:**
```
The Ramayana is an ancient Indian epic that tells the story of 
Prince Rama's quest to rescue his wife Sita from the demon king Ravana.
```

**Output:**
- **Cultural Origin**: Ancient Indian Sanskrit epic, part of Hindu Itihasa tradition, composed between 500-100 BCE
- **Cross-Cultural**: Influenced Southeast Asian cultures (Thailand's Ramakien, Indonesia's Wayang), spread through trade routes
- **Modern Analogy**: "Like the Marvel Cinematic Universe - it's a massive interconnected story that spawned countless adaptations, spin-offs, and regional versions across Asia. Both create a shared cultural vocabulary that millions reference."
- **Timeline Events**: 
  - 1500-1200 BCE: Oral tradition period
  - 500-400 BCE: Written Sanskrit version
  - 16th Century: Regional adaptations
- **Geographic Locations**: 
  - Ayodhya, India (26.7922¬∞ N, 82.1998¬∞ E)
  - Lanka (traditional identification with Sri Lanka)
- **Key Concepts**:
  - Dharma: Righteous duty and moral law
  - Avatar: Divine incarnation (Rama as Vishnu's avatar)
- **Detected Entities**: Ramayana (WORK_OF_ART), Rama (PERSON), Sita (PERSON), Ravana (PERSON) - all with Wikipedia tooltips

### Example 2: Japanese Poetry (Timeline + Geography + Concepts)
**Input:**
```
Haiku is a traditional form of Japanese poetry consisting of three 
lines with a 5-7-5 syllable pattern, often focusing on nature.
```

**Output:**
- **Cultural Origin**: Japanese Edo period poetry tradition (17th century), evolved from collaborative linked-verse
- **Cross-Cultural**: Adopted by Western poets, influenced Imagism movement, taught worldwide
- **Modern Analogy**: "Like Twitter's old 140-character limit, but for poetry. The strict 5-7-5 syllable structure forces you to capture a complete moment in the smallest space possible - similar to how TikTok creators tell entire stories in 15 seconds."
- **Timeline Events**:
  - 1600s: Matsuo Bash≈ç perfects haiku form
  - 1800s: Haiku spreads to Western literature
  - Modern: Taught in schools globally
- **Geographic Locations**: Edo (Tokyo), Japan (35.6762¬∞ N, 139.6503¬∞ E)
- **Key Concepts**:
  - Kigo: Seasonal reference
  - Kireji: Cutting word for emphasis
- **Detected Entities**: Haiku (WORK_OF_ART), Japanese (NORP) - with tooltips

### Example 3: European Renaissance (Enhanced Analogy + Resources)
**Input:**
```
The Renaissance was a period of cultural rebirth in Europe, marked 
by renewed interest in classical art, literature, and learning.
```

**Output:**
- **Cultural Origin**: 14th-17th century European cultural movement, began in Italian city-states
- **Cross-Cultural**: Spread from Italy to Northern Europe, influenced by Islamic Golden Age preservation of classical texts
- **Modern Analogy**: "Like the tech startup boom in Silicon Valley. Just as wealthy investors (VCs) fund innovative companies today, wealthy families like the Medici funded innovative artists and thinkers. Both eras saw rapid innovation, cross-pollination of ideas, and a few hotspot cities (Florence then, San Francisco/Seattle now) becoming centers of cultural transformation."
- **Timeline Events**:
  - 1300s: Proto-Renaissance in Italy
  - 1450s: Gutenberg printing press
  - 1500s: High Renaissance
- **Geographic Locations**: Florence, Venice, Rome (Italy)
- **Key Concepts**: 
  - Humanism: Human-centered philosophy
  - Perspective: Mathematical depth in art
- **External Resources**: Links to Khan Academy, interactive timelines
- **Detected Entities**: Renaissance (EVENT), Europe (LOC), Italy (GPE)

## Key Implementation Details

### Gemini API Integration
- Uses `gemini-2.5-flash` model for text analysis
- Structured JSON responses with conditional enhanced features
- Advanced prompt engineering for Gen Z/Millennial analogies
- Error handling with structured error objects
- JSON extraction via regex cleaning (handles markdown wrappers)
- Validation of required fields with empty array fallbacks

### NLP Pipeline (spaCy + Wikipedia)
- **Entity Extraction**: spaCy `en_core_web_sm` model
- **Entity Types**: PERSON, ORG, GPE, EVENT, WORK_OF_ART, FAC, NORP, LANGUAGE
- **Enrichment**: Wikipedia API with rate limiting (100ms intervals)
- **Caching**: Supabase entity_cache with 30-day TTL
- **Cultural Classification**: 9 categories based on Wikipedia categories
- **Performance**: First lookup ~1-2s, cached ~50ms

### Database Design (Supabase)
- Cloud PostgreSQL with JSONB support
- Supabase Python SDK (NOT raw SQL/psycopg2)
- `.table().insert()/.select()/.delete()` pattern
- GIN indexes on JSONB columns for performance
- No SQLAlchemy ORM - direct Supabase client
- Row Level Security (RLS) disabled for development

### Frontend Architecture
- Single-component React design (App.jsx)
- State management with useState hooks
- Collapsible sections: `showTimeline`, `showMap` control visibility
- Modal explainers: `expandedConcept` tracks active popup
- Interactive entity highlights with color-coded tooltips
- Responsive grid layouts with TailwindCSS
- ReactMarkdown for formatted text rendering

### Security Considerations
- Environment variables for sensitive data (Supabase keys, Gemini API)
- CORS `allow_origins=["*"]` (development mode)
- Input validation and sanitization
- No authentication (educational project)
- Rate limiting handled by external APIs

## Performance Optimizations

1. **NLP Caching**: Entity enrichment cached in Supabase (30-day TTL)
2. **Enrichment Limits**: Max 10 entities enriched per analysis to avoid long processing
3. **Rate Limiting**: 100ms between Wikipedia API calls
4. **JSONB Indexes**: GIN indexes for fast JSONB queries
5. **Frontend**: Code splitting, lazy loading components
6. **API**: Async/await for non-blocking operations
7. **Supabase**: Cloud-hosted with built-in connection pooling

## Future Enhancements

### High Priority
- [ ] Hugging Face transformers for semantic entity classification
- [ ] Entity relationship graph visualization
- [ ] Multi-language spaCy models (Spanish, French, etc.)
- [ ] Audio pronunciations for cultural terms
- [ ] Image integration from Wikipedia (entity photos)
- [ ] User authentication and personal libraries
- [ ] API rate limiting implementation

### Medium Priority
- [ ] Collaborative annotations
- [ ] Export to PDF/Word
- [ ] Advanced search and filtering
- [ ] Quiz generation from key concepts
- [ ] Interactive cultural maps (Leaflet/Mapbox)
- [ ] Timeline zoom/pan functionality
- [ ] Mobile app (React Native)

### Low Priority
- [ ] AR visualization options
- [ ] Social sharing of analyses
- [ ] Gamification (badges, progress tracking)
- [ ] Redis caching for frequent queries
- [ ] WebSocket for real-time updates
- [ ] GraphQL API alternative
- [ ] Docker containerization
- [ ] CI/CD pipeline
- [ ] Unit and integration tests

## Documentation Files

1. **README.md**: Main project documentation with v2.0/v3.0 features
2. **SETUP_GUIDE.md**: Detailed setup instructions with troubleshooting
3. **QUICKSTART.md**: 5-minute quick start guide
4. **SUPABASE_SETUP.md**: Complete Supabase setup guide
5. **ENHANCED_FEATURES.md**: v2.0 interactive features documentation
6. **NLP_ENRICHMENT_GUIDE.md**: Complete NLP module documentation
7. **NLP_QUICKSTART.md**: Quick start for NLP features
8. **PROJECT_SUMMARY.md**: This comprehensive overview
9. **COMMANDS_CHEATSHEET.md**: Quick reference for common commands
10. **DATABASE_COMMANDS.md**: Database management reference

## Commands Reference

### Database Commands (Supabase)
```sql
-- View all analyses
SELECT * FROM analyses ORDER BY created_at DESC LIMIT 10;

-- View analyses with entities
SELECT id, input_text, detected_entities 
FROM analyses 
WHERE detected_entities IS NOT NULL 
ORDER BY created_at DESC;

-- Count by language
SELECT language, COUNT(*) FROM analyses GROUP BY language;

-- View entity cache
SELECT entity_name, entity_type, cultural_significance 
FROM entity_cache 
ORDER BY created_at DESC;

-- Clear old cache entries
DELETE FROM entity_cache 
WHERE created_at < NOW() - INTERVAL '30 days';
```

### Development Commands
```powershell
# Backend
cd backend
.\venv\Scripts\Activate.ps1
python main.py

# Frontend
cd frontend
npm run dev

# Build frontend
npm run build

# NLP Setup
python -m spacy download en_core_web_sm

# Test NLP
python -c "from nlp_service import nlp_service; print(nlp_service.extract_entities('Test'))"
```

## Troubleshooting

### Common Issues

1. **Supabase Connection Failed**
   - Check `SUPABASE_URL` and `SUPABASE_ANON_KEY` in `.env`
   - Verify Supabase project is active
   - Test in Supabase Dashboard > SQL Editor first
   - Check internet connection (cloud service)

2. **Gemini API Error**
   - Verify API key is valid
   - Check internet connection
   - Review API quota limits
   - Check for safety filter blocks in logs

3. **spaCy Model Not Found**
   - Run: `python -m spacy download en_core_web_sm`
   - Verify installation: `python -c "import spacy; spacy.load('en_core_web_sm')"`

4. **No Entities Detected**
   - Check text length (minimum 10 characters)
   - Verify spaCy model is loaded
   - Check backend logs for NLP errors
   - Test with known entities (e.g., "Odysseus")

5. **Entity Cache Not Working**
   - Verify `entity_cache` table exists in Supabase
   - Check unique constraint on (entity_name, entity_type)
   - Review backend logs for Supabase errors

6. **Port Already in Use**
   - Backend: `netstat -ano | findstr :8000`
   - Frontend: `netstat -ano | findstr :5173`
   - Kill process: `taskkill /PID <PID> /F`

7. **JSON Parsing Errors (Gemini)**
   - Check backend logs for response preview
   - Gemini occasionally returns malformed JSON
   - Retry the request (usually temporary)
   - Check safety ratings in logs

## Dependencies

### Backend (Python)
```
fastapi>=0.104.0
uvicorn>=0.24.0
python-dotenv>=1.0.0
google-generativeai>=0.3.0
pillow>=10.0.0
python-multipart>=0.0.6
pydantic>=2.9.0
pydantic-settings>=2.5.0
supabase>=2.3.0              # Supabase Python SDK
postgrest>=0.13.0            # Supabase REST client

# NLP and Cultural Context Enrichment
spacy>=3.7.0                 # Named Entity Recognition
wikipedia-api>=0.7.0         # Wikipedia integration
transformers>=4.35.0         # Future: semantic classification
torch>=2.1.0                 # For transformers
requests>=2.31.0             # HTTP requests
```

### Frontend (Node.js)
```json
{
  "dependencies": {
    "axios": "^1.6.2",
    "lucide-react": "^0.294.0",
    "react": "^18.2.0",
    "react-dom": "^18.2.0",
    "react-markdown": "^10.1.0"
  },
  "devDependencies": {
    "@types/react": "^18.2.43",
    "@types/react-dom": "^18.2.17",
    "@vitejs/plugin-react": "^4.2.1",
    "autoprefixer": "^10.4.16",
    "postcss": "^8.4.32",
    "tailwindcss": "^3.3.6",
    "vite": "^5.0.8"
  }
}
```

### External APIs
- **Google Gemini API**: gemini-2.5-flash model
- **Supabase**: Cloud PostgreSQL database
- **Wikipedia API**: Entity enrichment via wikipedia-api library
- **Wikidata API**: Fallback for entity information

## License

MIT License - Free for educational and commercial use

## Support & Resources

- **Gemini API**: https://makersuite.google.com/
- **Supabase**: https://supabase.com/docs
- **FastAPI Docs**: https://fastapi.tiangolo.com/
- **React Docs**: https://react.dev/
- **spaCy Docs**: https://spacy.io/usage/linguistic-features
- **Wikipedia API**: https://wikipedia-api.readthedocs.io/
- **TailwindCSS**: https://tailwindcss.com/docs

## Conclusion

The Cultural Context Analyzer is a production-ready educational application that demonstrates:
- Modern full-stack development with cloud infrastructure (Supabase)
- Advanced AI integration (Gemini 2.5 Flash + conditional feature generation)
- NLP-powered entity detection with Wikipedia enrichment
- Interactive learning experiences (timelines, maps, tooltips, explainers)
- Contemporary pedagogical approaches (Gen Z/Millennial analogies)
- Clean architecture with separation of concerns
- Performance optimization through caching strategies
- Comprehensive documentation for maintenance and extension

**Perfect for**: Literature students, history learners, cultural studies, ESL education, anyone seeking to understand cultural contexts with interactive, verified background information.

**Key Innovations**:
1. **Conditional Enhanced Features** - Only generates timelines/maps/concepts when contextually relevant
2. **NLP Entity Detection** - Automatic identification and enrichment of cultural references
3. **Wikipedia Integration** - Verified, authoritative background summaries
4. **Cultural Significance Classification** - 9-category system for intelligent color-coding
5. **Modern Analogies** - Gen Z/Millennial references (social media, gaming, tech culture)
6. **Entity Caching** - 30-day TTL for performance optimization

---

**Created**: 2024  
**Version**: 3.0.0 (NLP Enrichment Update)  
**Status**: Production Ready ‚úÖ  
**Last Updated**: November 2025
